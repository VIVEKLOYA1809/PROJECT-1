{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize, special\n",
    "\n",
    "F = np.array([[1.0000, 0.1000],\n",
    "              [0, 1.0000]])\n",
    "\n",
    "G = np.array([[0.0050],\n",
    "              [0.1000]])\n",
    "\n",
    "C = np.array([[1, 0]])\n",
    "\n",
    "D = 0\n",
    "Gain = np.array([16.0302, 5.6622])\n",
    "\n",
    "L = np.array([[0.9902],\n",
    "              [0.9892]])\n",
    "safex = [26,31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=14\n",
    "inputs = np.random.normal(0, 1, k)\n",
    "input_len=len(inputs)\n",
    "X = np.zeros((2, input_len + 1))\n",
    "Xe = np.zeros((2, input_len + 1))\n",
    "e = np.zeros((2, input_len))\n",
    "U = np.zeros((1,input_len))\n",
    "Y = np.zeros((1,input_len))\n",
    "r = np.zeros((1,input_len))\n",
    "z = np.zeros((1,input_len))\n",
    "S = np.zeros((input_len))\n",
    "n1 = np.random.normal(0,0.01, [1, input_len])      \n",
    "n2 = np.random.normal(0,0.001, [2, input_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(input_len):\n",
    "    e[:, i] = X[:, i] - Xe[:, i]\n",
    "    U[:,i] = -(Gain @ Xe[:, i])\n",
    "    X[:, i+1] = F @ X[:, i] + G @ U[:, i] + n2[:, i]\n",
    "    Y[:,i] = C @ X[:, i] + n1[:,i]\n",
    "    r[:,i] = Y[:,i] - C @ Xe[:, i]\n",
    "    Xe[:, i+1] = F @ Xe[:, i] + G @ U[:,i] + L @ r[:,i]\n",
    "covn_r=np.cov(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackK= [0, 0.04155838 , 0.63838859 , 1.75426837 ,2.44935936 , 3.6651983 , 5.16561739 , 6.95368618 , 9.11716362 ,11.67267658 ,14.29706254, 17.41085821  ,0.       ,   0.    ]\n",
    "k=len(attackK)\n",
    "attack2K = [-30.971675   ,-30.971675  ,  -1.11684483 ,-30.90514398, -28.29714574 , -28.61888554, -28.91105434, -29.17567371, -29.41469648, -29.21724115 , -28.49741602 , 30.971675,     0.     ,      0. ]\n",
    "k1=len(attack2K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.normal(0, 1, k)\n",
    "input_len=len(inputs)\n",
    "Xk = np.zeros((2, input_len + 1))\n",
    "Xek = np.zeros((2, input_len + 1))\n",
    "ek = np.zeros((2, input_len))\n",
    "Uk = np.zeros((1,input_len))\n",
    "Uak = np.zeros((1,input_len))\n",
    "Yk = np.zeros((1,input_len))\n",
    "rk = np.zeros((1,input_len))\n",
    "covn_eK = np.zeros((2, 2))\n",
    "covn_rK = np.zeros((2, 2))\n",
    "zk = np.zeros((1,input_len))\n",
    "Sk = np.zeros((input_len))\n",
    "n1 = np.random.normal(0,0.01, [1, input_len])      \n",
    "n2 = np.random.normal(0,0.001, [2,input_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuek=[]\n",
    "mzk=[]\n",
    "bias=4\n",
    "for i in range(input_len):\n",
    "    ek[:, i] = Xk[:, i] - Xek[:, i]\n",
    "    Uk[:,i] = -(Gain @ Xek[:, i])\n",
    "    Uak[:,i] = -(Gain @ Xek[:, i])+attack2K[i]\n",
    "    Xk[:, i+1] = F @ Xk[:, i] + G @ Uak[:, i] + n2[:, i]\n",
    "    Yk[:,i] = C @ Xk[:, i] + n1[:,i] + attackK[i]\n",
    "    rk[:,i] = Yk[:,i] - C @ Xek[:, i]\n",
    "    Xek[:, i+1] = F @ Xek[:, i] + G @ Uk[:,i] + L @ rk[:,i]\n",
    "    zk[:,i] = rk[:,i].T * covn_r * rk[:,i]\n",
    "    residuek.append(rk[:,i])\n",
    "    mzk.append(zk[:,i])\n",
    "    Sk[i] = max((Sk[i-1] + zk[:,i] - bias), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.mean(np.array(mzk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def discrete_frame(Sk,N,th):\n",
    "    delSi = 2*th/(2*N-1)\n",
    "    Ek=np.zeros((len(Sk)))\n",
    "    for i in range(len(Sk)):\n",
    "        if Sk[i]<delSi/2:\n",
    "            Ek[i]=0\n",
    "        elif Sk[i]>=th:\n",
    "            Ek[i]=N\n",
    "        else :\n",
    "            Ek[i]=math.ceil(math.floor(Sk[i]/(delSi/2))/2)\n",
    "    return Ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuek=np.absolute(np.array(residuek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mean=np.array([])\n",
    "for i in range(len(residuek)):\n",
    "    sums=0\n",
    "    for j in range(i+1):\n",
    "        sums+=residuek[j]\n",
    "    mean=sums/(j+1)\n",
    "    r_mean=np.append(r_mean,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00942689, 0.06524382, 0.08776539, 0.19433786, 0.17663647,\n",
       "       0.16298772, 0.14877472, 0.14244312, 0.14558693, 0.15635856,\n",
       "       0.14310531, 0.14724148, 1.70031137, 1.79228913])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(r_mean>0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "import scipy.special as special\n",
    "def markov_transition(N, th, bias):\n",
    "    temp = np.zeros((N+1, N+1))\n",
    "    for i in range(1,N+1):\n",
    "        for j in range(1,N+1):\n",
    "            if j == 1 & i <= N:\n",
    "                temp[i, j] = Tx(1-i, th, bias, N)\n",
    "            elif j > 1 & j < N+1 & i <= N:\n",
    "                temp[i, j] = Px(j-i, th, bias, N)\n",
    "            elif j == N+1 & i<=N:\n",
    "                temp[i, j] = 1 - Tx(N-i, th, bias, N)\n",
    "            elif i == N+1 & j < N+1:\n",
    "                temp[i, j] = 0\n",
    "            else:\n",
    "                temp[i, j] = 1\n",
    "    P = temp\n",
    "    return P\n",
    "\n",
    "def Px(x, th, bias, N):\n",
    "    delSi = 2*th/(2*N-1)\n",
    "    z1 = np.max(x*delSi + 0.5*delSi + bias, 0)\n",
    "    p1 = special.gammainc(m/2, z1/2)\n",
    "    z2 = np.max(x*delSi - 0.5*delSi + bias, 0)\n",
    "    p2 = special.gammainc(m/2, z2/2)\n",
    "    p = p1-p2\n",
    "    return p\n",
    "\n",
    "def Tx(x, th, bias, N):\n",
    "    delSi = 2*th/(2*N-1)\n",
    "    z = np.max(x*delSi + 0.5*delSi + bias, 0)\n",
    "    t = special.gammainc(m/2, z/2)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def calculate_reward(cusum,residual,i):\n",
    "        # Calculate reward based on attack detection condition\n",
    "        if residual[i]>0.01 and cusum[i]> threshold:\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(grad_func, initial_values, learning_rate=0.8, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1):\n",
    "    p = np.zeros_like(initial_values)\n",
    "    v = np.zeros_like(initial_values)\n",
    "    t = 0\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        t += 1\n",
    "        gradient = grad_func(initial_values)\n",
    "        p = beta1 * p + (1 - beta1) * gradient\n",
    "        v = beta2 * v + (1 - beta2) * (gradient ** 2)\n",
    "        p_hat = p / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        initial_values += learning_rate * p_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special, optimize\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params,m,N,E):\n",
    "    th, bias = params\n",
    "    x=N-1-Ek\n",
    "    delSi = 2 * th / (2 * N - 1)\n",
    "    z = np.maximum(x * delSi + 0.5 * delSi + bias, 0)\n",
    "    t = special.gammainc(m / 2, z / 2)-1\n",
    "    return -t  # Negate t since we want to maximize it\n",
    "\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(params):\n",
    "    th, bias = params\n",
    "    x=N-1-Ek\n",
    "    delSi = 2 * th / (2 * N - 1)\n",
    "    z = np.maximum(x * delSi + 0.5 * delSi + bias, 0)\n",
    "    t = 1 - special.gammainc(m / 2, z / 2)\n",
    "    dt_dth = 0.5 * z * np.exp(-z / 2) * x / (2 * N - 1)\n",
    "    dt_dbias = 0.5 * z * np.exp(-z / 2)\n",
    "    return np.array([-dt_dth, -dt_dbias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_space = np.random.uniform(1,6,6)\n",
    "# action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RLEnvironment():\n",
    "    def __init__(self, num_states,num_action,cusum,residual,threshold,pos):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_action\n",
    "        self.cusum = cusum\n",
    "        self.residual = residual\n",
    "        self.threshold = threshold\n",
    "        self.current_state = None\n",
    "        \n",
    "    def discrete_frame(self,N):\n",
    "        delSi = 2*self.threshold/(2*N-1)\n",
    "        Ek=np.zeros((len(self.cusum)))\n",
    "        for i in range(len(self.cusum)):\n",
    "            if self.cusum[i]<delSi/2:\n",
    "                Ek[i]=0\n",
    "            elif self.cusum[i]>=self.threshold:\n",
    "                Ek[i]=N\n",
    "            else :\n",
    "                Ek[i]=math.ceil(math.floor(self.cusum[i]/(delSi/2))/2)\n",
    "        return Ek.reshape(1,-1)\n",
    "        \n",
    "    def reset(self,N):\n",
    "        self.current_state = self.discrete_frame(N)\n",
    "        return self.current_state\n",
    "    \n",
    "    def discreted_frame(self,cusum,N,threshold,pos):\n",
    "        delSi = 2*threshold/(2*N-1)\n",
    "        if cusum[pos] < delSi/2:\n",
    "            Ek=0\n",
    "        elif cusum[pos]>=th:\n",
    "            Ek=N\n",
    "        else :\n",
    "            Ek=math.ceil(math.floor(cusum[pos]/(delSi/2))/2)\n",
    "        return Ek\n",
    "    \n",
    "    def calculate_reward(self,cusum,residual,threshold,i):\n",
    "        # Calculate reward based on attack detection condition\n",
    "        if residual[i]>0.01 and cusum[i]> threshold:\n",
    "            return 10\n",
    "        elif residual[i]<0.01 and cusum[i]<threshold:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def step(self,cusum,N,pos,threshold):\n",
    "        \n",
    "        next_state = self.discrete_frame(N)\n",
    "        \n",
    "        reward = self.calculate_reward(cusum,self.residual,threshold,pos)\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        done = (self.current_state == N)\n",
    "        \n",
    "        return next_state, reward, done,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define actor and critic neural network architectures\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.output_layer = nn.Linear(300, action_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = torch.tanh(self.output_layer(x))\n",
    "        return actions\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.output_layer = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.output_layer(x)\n",
    "        return q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "threshold = 6\n",
    "pos=0\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "LR_ACTOR = 0.0001\n",
    "LR_CRITIC = 0.001\n",
    "num_action=2\n",
    "env = RLEnvironment(len(Sk),num_action,Sk,r_mean,threshold,pos)\n",
    "\n",
    "# Initialize actor, critic, target actor, and target critic networks\n",
    "state_dim = env.num_states\n",
    "action_dim = env.num_actions\n",
    "\n",
    "# Create actor and critic networks\n",
    "actor = ActorNetwork(state_dim, action_dim)\n",
    "critic = CriticNetwork(state_dim, action_dim)\n",
    "\n",
    "# Create target actor and critic networks and initialize them with main network weights\n",
    "target_actor = copy.deepcopy(actor)\n",
    "target_critic = copy.deepcopy(critic)\n",
    "\n",
    "# Define optimizer for actor and critic networks\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "\n",
    "# Other components like replay buffer, training loop, exploration noise, etc.\n",
    "# should be implemented and integrated into this code as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[array([5.20000036, 3.20000006])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([4.40000062, 2.4000001 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([3.60000081, 1.60000013])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([2.80000096, 0.80000015])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([2.00000110e+00, 1.73020919e-07])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\AppData\\Local\\Temp/ipykernel_30108/3053933552.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 1 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 2 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 3 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 4 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 5 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 6 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 7 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 8 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 9 - Reward: 14\n",
      "[[ 0.  1.  3.  4.  5.  6.  8.  9. 10. 10. 10. 10. 10. 10.]]\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "tensor([[ 0.,  1.,  3.,  4.,  5.,  6.,  8.,  9., 10., 10., 10., 10., 10., 10.]])\n",
      "[array([ 1.20000129, -0.7999998 ])]\n",
      "Episode 10 - Reward: 14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define other hyperparameters and initialization steps\n",
    "learning_rate_actor = 0.001\n",
    "learning_rate_critic = 0.001\n",
    "learning_rate_threshold = 0.01  # Learning rate for the threshold optimizer\n",
    "# batch_size = 64\n",
    "buffer_size = 10000\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "num_episodes = 10\n",
    "N=10\n",
    "batch_size=1\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = []\n",
    "initial_values = [6,4]\n",
    "# Main training loop\n",
    "action=initial_values\n",
    "for episode in range(num_episodes):\n",
    "    states = env.reset(N)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for i in range(len(Sk)):\n",
    "        # Select action using actor network\n",
    "        pos=i\n",
    "        action = adam_optimizer(gradient, action)\n",
    "        actions=[action]\n",
    "        threshold=action[0]\n",
    "        bias=action[1]\n",
    "        \n",
    "        \n",
    "        # Perform action in the environment\n",
    "        next_states, rewards, dones = env.step(Sk,N,pos,threshold)\n",
    "        print(states)\n",
    "        print(actions)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Update critic network\n",
    "        q_values = critic(states, actions)\n",
    "        next_actions = target_actor(next_states)\n",
    "        next_q_values = target_critic(next_states, next_actions)\n",
    "        target_q_values = rewards + gamma * (1 - dones) * next_q_values\n",
    "        critic_loss = nn.MSELoss()(q_values, target_q_values.detach())\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Update actor network\n",
    "        actor_loss = -critic(states, actor(states)).mean()\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        \n",
    "        for i in range(1,len(mzk)):\n",
    "            Sk[i]=max(Sk[i-1]+mzk[i]-bias,0)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "\n",
    "        # Update target networks using soft updates\n",
    "        for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    print(f\"Episode {episode+1} - Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the trained actor network for generating actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# # Define actor and critic neural network architectures\n",
    "# class ActorNetwork(tf.keras.Model):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(ActorNetwork, self).__init__()\n",
    "#         self.fc1 = Dense(400, activation='relu')\n",
    "#         self.fc2 = Dense(300, activation='relu')\n",
    "#         self.output_layer = Dense(action_dim, activation='tanh')\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.fc1(inputs)\n",
    "#         x = self.fc2(x)\n",
    "#         actions = self.output_layer(x)\n",
    "#         return actions\n",
    "\n",
    "# class CriticNetwork(tf.keras.Model):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(CriticNetwork, self).__init__()\n",
    "#         self.fc1 = Dense(400, activation='relu')\n",
    "#         self.fc2 = Dense(300, activation='relu')\n",
    "#         self.output_layer = Dense(1)\n",
    "\n",
    "#     def call(self, state, action):\n",
    "#         x = tf.concat([state, action], axis=-1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         q_value = self.output_layer(x)\n",
    "#         return q_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
